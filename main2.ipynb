{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15150, 512) (15150,)\n",
      "(5050, 100)\n",
      "(5050,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "from data_utils import Data, ImbalancedData\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def extract_feature(dataloader, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            img, label = data\n",
    "            img = img.to(device)\n",
    "            feature = model(img)\n",
    "            features.append(feature.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features.squeeze(), labels\n",
    "\n",
    "def evaluation(label, pred):\n",
    "    acc = accuracy_score(label, pred)\n",
    "    f1 = f1_score(label, pred, average='macro', zero_division=0)\n",
    "    precision = precision_score(label, pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(label, pred, average='macro', zero_division=0)\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize((224, 224))])  # ResNet 的輸入尺寸\n",
    "dataset = ImbalancedData('./Mini', train_n=300, test_n=100, transform=transform)\n",
    "# dataset = Data('./Mini', train_n=50, test_n=10, transform=transform)\n",
    "\n",
    "train_data, test_data = dataset.train_data, dataset.test_data\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=10)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=10)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cnn_model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "cnn_model = nn.Sequential(*list(cnn_model.children())[:-1]).to(device)\n",
    "\n",
    "train_feature, train_label = extract_feature(train_loader, cnn_model)\n",
    "test_feature, test_label = extract_feature(test_loader, cnn_model)\n",
    "print(train_feature.shape, train_label.shape)\n",
    "\n",
    "kmean = KMeans(n_clusters=100)\n",
    "kmean.fit(train_feature)\n",
    "\n",
    "test_kmean_features = kmean.transform(test_feature)\n",
    "print(test_kmean_features.shape)\n",
    "test_pred = np.argmin(test_kmean_features, axis=1)\n",
    "test_dist = np.min(test_kmean_features, axis=1)\n",
    "print(test_dist.shape)\n",
    "\n",
    "test_std = np.zeros((len(test_feature)))\n",
    "for i in range(100):\n",
    "    in_class_idx = np.where(test_pred==i)\n",
    "    in_class_mean = np.mean(test_dist[in_class_idx], axis=0)\n",
    "    in_class_std = np.std(test_dist[in_class_idx], axis=0)\n",
    "    if in_class_std == 0:\n",
    "        print(f'Class {i} has std = 0')\n",
    "\n",
    "    test_std[in_class_idx] = np.abs( test_dist[in_class_idx] - in_class_mean) / in_class_std\n",
    "\n",
    "\n",
    "# svm = SVC(probability=True, C=1.0, kernel='rbf')\n",
    "svm_norm = SVC(probability=True, C=1, kernel='rbf')\n",
    "svm_imba = SVC(probability=True, C=2, kernel='rbf')\n",
    "svm_norm.fit(train_feature, train_label)\n",
    "svm_imba.fit(train_feature, train_label)\n",
    "\n",
    "svm_norm_prob = svm_norm.predict_proba(test_feature)\n",
    "svm_imba_prob = svm_imba.predict_proba(test_feature)\n",
    "svm_norm_pred = np.argmax(svm_norm_prob, axis=1)\n",
    "svm_imba_pred = np.argmax(svm_imba_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62       100\n",
      "           1       0.61      0.66      0.63        99\n",
      "           2       0.88      0.86      0.87        98\n",
      "           3       0.56      0.75      0.64        97\n",
      "           4       0.70      0.78      0.74        96\n",
      "           5       0.73      0.71      0.72        95\n",
      "           6       0.61      0.66      0.63        94\n",
      "           7       0.60      0.72      0.66        93\n",
      "           8       0.72      0.83      0.77        92\n",
      "           9       0.61      0.66      0.63        91\n",
      "          10       0.57      0.57      0.57        90\n",
      "          11       0.76      0.80      0.78        89\n",
      "          12       0.53      0.61      0.57        88\n",
      "          13       0.58      0.67      0.62        87\n",
      "          14       0.64      0.70      0.67        86\n",
      "          15       0.63      0.69      0.66        85\n",
      "          16       0.52      0.64      0.58        84\n",
      "          17       0.55      0.64      0.59        83\n",
      "          18       0.49      0.57      0.53        82\n",
      "          19       0.53      0.62      0.57        81\n",
      "          20       0.71      0.46      0.56        80\n",
      "          21       0.52      0.44      0.48        79\n",
      "          22       0.81      0.74      0.77        78\n",
      "          23       0.86      0.83      0.85        77\n",
      "          24       0.55      0.43      0.49        76\n",
      "          25       0.48      0.45      0.47        75\n",
      "          26       0.60      0.61      0.60        74\n",
      "          27       0.56      0.66      0.61        73\n",
      "          28       0.64      0.68      0.66        72\n",
      "          29       0.57      0.42      0.48        71\n",
      "          30       0.74      0.76      0.75        70\n",
      "          31       0.73      0.81      0.77        69\n",
      "          32       0.57      0.68      0.62        68\n",
      "          33       0.54      0.42      0.47        67\n",
      "          34       0.85      0.83      0.84        66\n",
      "          35       0.57      0.72      0.64        65\n",
      "          36       0.62      0.61      0.61        64\n",
      "          37       0.66      0.52      0.58        63\n",
      "          38       0.65      0.68      0.66        62\n",
      "          39       0.66      0.41      0.51        61\n",
      "          40       0.73      0.75      0.74        60\n",
      "          41       0.70      0.71      0.71        59\n",
      "          42       0.49      0.50      0.50        58\n",
      "          43       0.67      0.77      0.72        57\n",
      "          44       0.52      0.57      0.54        56\n",
      "          45       0.51      0.69      0.58        55\n",
      "          46       0.83      0.81      0.82        54\n",
      "          47       0.54      0.64      0.59        53\n",
      "          48       0.53      0.52      0.52        52\n",
      "          49       0.81      0.67      0.73        51\n",
      "          50       0.52      0.74      0.61        50\n",
      "          51       0.68      0.55      0.61        49\n",
      "          52       0.58      0.69      0.63        48\n",
      "          53       0.47      0.49      0.48        47\n",
      "          54       0.53      0.52      0.53        46\n",
      "          55       0.75      0.60      0.67        45\n",
      "          56       0.67      0.66      0.67        44\n",
      "          57       0.81      0.81      0.81        43\n",
      "          58       0.60      0.64      0.62        42\n",
      "          59       0.71      0.71      0.71        41\n",
      "          60       0.63      0.42      0.51        40\n",
      "          61       0.66      0.59      0.62        39\n",
      "          62       0.70      0.61      0.65        38\n",
      "          63       0.71      0.78      0.74        37\n",
      "          64       0.56      0.56      0.56        36\n",
      "          65       0.84      0.46      0.59        35\n",
      "          66       0.69      0.71      0.70        34\n",
      "          67       0.58      0.42      0.49        33\n",
      "          68       0.54      0.47      0.50        32\n",
      "          69       0.61      0.55      0.58        31\n",
      "          70       0.58      0.47      0.52        30\n",
      "          71       0.50      0.45      0.47        29\n",
      "          72       0.55      0.43      0.48        28\n",
      "          73       0.68      0.63      0.65        27\n",
      "          74       0.58      0.42      0.49        26\n",
      "          75       0.44      0.32      0.37        25\n",
      "          76       0.44      0.50      0.47        24\n",
      "          77       0.67      0.26      0.38        23\n",
      "          78       1.00      0.05      0.09        22\n",
      "          79       0.59      0.48      0.53        21\n",
      "          80       0.62      0.50      0.56        20\n",
      "          81       0.58      0.37      0.45        19\n",
      "          82       0.88      0.78      0.82        18\n",
      "          83       0.50      0.18      0.26        17\n",
      "          84       0.86      0.38      0.52        16\n",
      "          85       0.56      0.33      0.42        15\n",
      "          86       0.57      0.29      0.38        14\n",
      "          87       0.38      0.23      0.29        13\n",
      "          88       0.50      0.25      0.33        12\n",
      "          89       0.00      0.00      0.00        11\n",
      "          90       0.75      0.30      0.43        10\n",
      "          91       0.27      0.44      0.33         9\n",
      "          92       0.40      0.25      0.31         8\n",
      "          93       1.00      0.29      0.44         7\n",
      "          94       1.00      0.33      0.50         6\n",
      "          95       0.00      0.00      0.00         5\n",
      "          96       0.00      0.00      0.00         4\n",
      "          97       0.00      0.00      0.00         3\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      5050\n",
      "   macro avg       0.59      0.54      0.55      5050\n",
      "weighted avg       0.63      0.62      0.62      5050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(test_label, svm_imba_pred, zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accuracies = {}\n",
    "for class_idx in range(100):\n",
    "    # 找到屬於當前類別的樣本索引\n",
    "    class_indices = np.where(test_label == class_idx)[0]\n",
    "    # 計算當前類別的準確率\n",
    "    if len(class_indices) > 0:\n",
    "        class_accuracy = accuracy_score(test_label[class_indices], svm_norm_pred[class_indices])\n",
    "        class_accuracies[f'Class {class_idx}'] = class_accuracy\n",
    "    else:\n",
    "        class_accuracies[f'Class {class_idx}'] = None  # 或者設置為其他適當的值\n",
    "\n",
    "# 打印每個類別的準確率\n",
    "for class_name, class_accuracy in class_accuracies.items():\n",
    "       print(f'{class_name}: {class_accuracy:.4f}' if class_accuracy is not None else f'{class_name}: No samples')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eating",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
